{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import LabelEncoder\n",
    "import os\n",
    "import ipaddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge les données et on encode les variable catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=pd.read_csv(\"../data/final/Credential_Access.csv\")\n",
    "df1=pd.read_csv(\"../data/final/Defense_Evasion.csv\")\n",
    "df2=pd.read_csv(\"../data/final/Discovery.csv\")\n",
    "df3=pd.read_csv(\"../data/final/Exfiltration.csv\")\n",
    "df4=pd.read_csv(\"../data/final/Initial_Access.csv\")\n",
    "df5=pd.read_csv(\"../data/final/none.csv\")\n",
    "df6=pd.read_csv(\"../data/final/Persistence.csv\")\n",
    "df7=pd.read_csv(\"../data/final/Privilege_Escalation.csv\")\n",
    "df8=pd.read_csv(\"../data/final/Reconnaissance.csv\")\n",
    "listeDatasets=[df0,df1,df2,df3,df4,df5,df6,df7,df8]\n",
    "\n",
    "categorical_columns = ['conn_state', 'protocol', 'service', 'history', 'label_tactic']\n",
    "\n",
    "\n",
    "#Gestion des dates\n",
    "for i in range(len(listeDatasets)):\n",
    "    df=listeDatasets[i]\n",
    "    df['ts'] = pd.to_datetime(df['ts'], unit='s')\n",
    "    df['year'] = df['ts'].dt.year\n",
    "    df['month'] = df['ts'].dt.month\n",
    "    df['day'] = df['ts'].dt.day\n",
    "    df['hour'] = df['ts'].dt.hour\n",
    "    df['minute'] = df['ts'].dt.minute\n",
    "    df['seconde'] = df['ts'].dt.second\n",
    "    df['dayofweek'] = df['ts'].dt.dayofweek  # 0=lundi, 6=dimanche\n",
    "\n",
    "    # Caractéristiques cycliques pour l'heure (pour préserver la nature cyclique)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "\n",
    "    # Caractéristiques cycliques pour le jour de la semaine\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)\n",
    "\n",
    "    # Trier par timestamp pour calculer les différences temporelles\n",
    "    df = df.sort_values('ts')\n",
    "    df['time_since_last'] = df['ts'].diff().dt.total_seconds()\n",
    "\n",
    "    # Remplacer la première valeur NaN par 0 ou une autre valeur appropriée\n",
    "    df['time_since_last'] = df['time_since_last'].fillna(0)\n",
    "\n",
    "    df=df.drop(['ts'], axis=1)\n",
    "\n",
    "    #Gestion des adresses IP\n",
    "    df['src_ip'] = df['src_ip'].apply(lambda x: int(ipaddress.ip_address(x)))\n",
    "    df['dest_ip'] = df['dest_ip'].apply(lambda x: int(ipaddress.ip_address(x)))\n",
    "    \n",
    "    #Une IPV6 est un trop grand nombre, on ne peut pas le mettre en entier\n",
    "    #On normalise donc les ip en divisant par la plus grande valeur possible (pour etre entre 0 et 1)\n",
    "    df['src_ip'] = df['src_ip']/(2**128 - 1) # 2^128 est le max pour IPv6 car codée sur 128 bits\n",
    "    df['dest_ip'] = df['dest_ip']/(2**128 - 1)\n",
    "\n",
    "    df['src_ip']=df['src_ip'].astype(float)\n",
    "    df['dest_ip']=df['dest_ip'].astype(float)\n",
    "\n",
    "    \"\"\"\n",
    "    # Séparation de l'adresse IP en 4 colonnes\n",
    "    df[['ip1', 'ip2', 'ip3', 'ip4']] = df['src_ip'].str.split('.', expand=True)\n",
    "\n",
    "    # Conversion en entiers (optionnel)\n",
    "    df[['ip1', 'ip2', 'ip3', 'ip4']] = df[['ip1', 'ip2', 'ip3', 'ip4']].astype(int)\n",
    "\n",
    "    listeDatasets[i]=df.drop(['src_ip'], axis=1)\n",
    "    \"\"\"\n",
    "    listeDatasets[i]=df\n",
    "# Créer et stocker les encodeurs\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    # Collecter toutes les valeurs uniques de TOUS les datasets\n",
    "    all_values = pd.concat([df[col] for df in listeDatasets]).unique()\n",
    "    \n",
    "    # Créer et ajuster l'encodeur sur toutes les valeurs possibles\n",
    "    encoders[col] = LabelEncoder().fit(all_values)\n",
    "    \n",
    "    # Appliquer l'encodeur à chaque dataset\n",
    "    for df in listeDatasets:\n",
    "        df[col] = encoders[col].transform(df[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de la structure d'un dataset encodé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['conn_state', 'duration', 'local_orig', 'local_resp', 'protocol',\n",
      "       'service', 'history', 'src_ip', 'src_port', 'orig_bytes', 'orig_pkts',\n",
      "       'orig_ip_bytes', 'dest_ip', 'dest_port', 'resp_bytes', 'resp_pkts',\n",
      "       'resp_ip_bytes', 'missed_bytes', 'label_tactic', 'year', 'month', 'day',\n",
      "       'hour', 'minute', 'seconde', 'dayofweek', 'hour_sin', 'hour_cos',\n",
      "       'dow_sin', 'dow_cos', 'time_since_last'],\n",
      "      dtype='object')\n",
      "291    7.067394e-30\n",
      "318    7.067402e-30\n",
      "279    7.067397e-30\n",
      "268    7.067402e-30\n",
      "253    7.067402e-30\n",
      "Name: dest_ip, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(listeDatasets[1].columns)\n",
    "print(listeDatasets[7][\"dest_ip\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1712479, 31)\n",
      "(428126, 31)\n",
      "Index(['conn_state', 'duration', 'local_orig', 'local_resp', 'protocol',\n",
      "       'service', 'history', 'src_ip', 'src_port', 'orig_bytes', 'orig_pkts',\n",
      "       'orig_ip_bytes', 'dest_ip', 'dest_port', 'resp_bytes', 'resp_pkts',\n",
      "       'resp_ip_bytes', 'missed_bytes', 'label_tactic', 'year', 'month', 'day',\n",
      "       'hour', 'minute', 'seconde', 'dayofweek', 'hour_sin', 'hour_cos',\n",
      "       'dow_sin', 'dow_cos', 'time_since_last'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Diviser chaque dataset en train/test (80/20)\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "for df in listeDatasets:\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_dfs.append(train_df)\n",
    "    test_dfs.append(test_df)\n",
    "\n",
    "#Concaténer les ensembles\n",
    "combined_train = pd.concat(train_dfs)\n",
    "combined_test = pd.concat(test_dfs)\n",
    "\n",
    "print(combined_train.shape)\n",
    "print(combined_test.shape)\n",
    "print(combined_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de confusion:\n",
      "[[  8189      1      2      7    106      7     36     46    305]\n",
      " [     1      9      0      0     19     12     14      4      7]\n",
      " [     0      0    387      0      0      0      1      1     29]\n",
      " [     0      0      0      0      0      1      1      1      2]\n",
      " [     2     15      0      0     64      5     16      4      7]\n",
      " [     1      9      0      0     19     12     14      4      7]\n",
      " [     1      9      0      0     19     12     14      4      7]\n",
      " [    31    142      6      1     19     58     20 100345    875]\n",
      " [    32     36    273     11    276     48    400   2329 313791]]\n",
      "\n",
      "Rapport de classification:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "   Credential Access       0.99      0.94      0.97      8699\n",
      "     Defense Evasion       0.04      0.14      0.06        66\n",
      "           Discovery       0.58      0.93      0.71       418\n",
      "        Exfiltration       0.00      0.00      0.00         5\n",
      "      Initial Access       0.12      0.57      0.20       113\n",
      "         Persistence       0.08      0.18      0.11        66\n",
      "Privilege Escalation       0.03      0.21      0.05        66\n",
      "      Reconnaissance       0.98      0.99      0.98    101497\n",
      "                none       1.00      0.99      0.99    317196\n",
      "\n",
      "            accuracy                           0.99    428126\n",
      "           macro avg       0.42      0.55      0.45    428126\n",
      "        weighted avg       0.99      0.99      0.99    428126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['conn_state', 'duration', 'local_orig', 'local_resp', 'protocol',\n",
    "       'service', 'history','src_ip',  'src_port', 'orig_bytes', 'orig_pkts',\n",
    "       'orig_ip_bytes', 'dest_ip', 'dest_port', 'resp_bytes', 'resp_pkts',\n",
    "       'resp_ip_bytes', 'missed_bytes', 'year', 'month', 'day',\n",
    "       'hour', 'minute', 'seconde', 'dayofweek', 'hour_sin', 'hour_cos',\n",
    "       'dow_sin', 'dow_cos', 'time_since_last']\n",
    "\n",
    "label='label_tactic'\n",
    "\n",
    "# Préparer X et y\n",
    "X_train = combined_train[features]\n",
    "y_train = combined_train[label]\n",
    "X_test = combined_test[features]\n",
    "y_test = combined_test[label]\n",
    "\n",
    "# Créer les datasets LightGBM\n",
    "categorical_indices = [features.index(col) for col in categorical_columns[:-1] ]#On recupere l'index de chaque colonne catégorielle (sauf le label)\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_indices)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "\n",
    "\n",
    "# 9. Paramètres LightGBM\n",
    "num_classes = len(encoders['label_tactic'].classes_)\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': num_classes,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 10. Entraîner le modèle\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[test_data],\n",
    ")\n",
    "\n",
    "# 11. Évaluer le modèle\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Reconvertir en labels textuels pour l'évaluation\n",
    "y_pred_labels = encoders['label_tactic'].inverse_transform(y_pred_class)\n",
    "y_test_labels = encoders['label_tactic'].inverse_transform(y_test)\n",
    "\n",
    "# Afficher les résultats\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nMatrice de confusion:\")\n",
    "print(confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "\n",
    "# Sauvegarder le modèle et les encodeurs\n",
    "import pickle\n",
    "model_package = {\n",
    "    'model': gbm,\n",
    "    'encoders': encoders,\n",
    "    'features': features\n",
    "}\n",
    "\n",
    "with open('lightgbm_model_package.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['conn_state_encoded', 'protocol_encoded', 'service_encoded', 'history_encoded'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m features \u001b[38;5;241m=\u001b[39m numerical_features \u001b[38;5;241m+\u001b[39m encoded_categorical\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 7. Préparer X et y\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     29\u001b[0m y_train \u001b[38;5;241m=\u001b[39m combined_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_tactic_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     30\u001b[0m X_test \u001b[38;5;241m=\u001b[39m combined_test[features]\n",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['conn_state_encoded', 'protocol_encoded', 'service_encoded', 'history_encoded'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 8. Créer les datasets LightGBM\n",
    "categorical_indices = [features.index(col) for col in encoded_categorical]\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_indices)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 9. Paramètres LightGBM\n",
    "num_classes = len(encoders['label_tactic'].classes_)\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': num_classes,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 10. Entraîner le modèle\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[test_data],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# 11. Évaluer le modèle\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Reconvertir en labels textuels pour l'évaluation\n",
    "y_pred_labels = encoders['label_tactic'].inverse_transform(y_pred_class)\n",
    "y_test_labels = encoders['label_tactic'].inverse_transform(y_test)\n",
    "\n",
    "# Afficher les résultats\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nMatrice de confusion:\")\n",
    "print(confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "\n",
    "# Sauvegarder le modèle et les encodeurs\n",
    "import pickle\n",
    "model_package = {\n",
    "    'model': gbm,\n",
    "    'encoders': encoders,\n",
    "    'features': features\n",
    "}\n",
    "\n",
    "with open('lightgbm_model_package.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        src_ip  ip1  ip2  ip3  ip4\n",
      "0  143.88.1.18  143   88    1   18\n",
      "1  192.168.0.1  192  168    0    1\n",
      "2   10.0.0.255   10    0    0  255\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exemple de DataFrame\n",
    "dfZZ = pd.DataFrame({'src_ip': ['143.88.1.18', '192.168.0.1', '10.0.0.255']})\n",
    "\n",
    "# Séparation de l'adresse IP en 4 colonnes\n",
    "dfZZ[['ip1', 'ip2', 'ip3', 'ip4']] = dfZZ['src_ip'].str.split('.', expand=True)\n",
    "\n",
    "# Conversion en entiers (optionnel)\n",
    "dfZZ[['ip1', 'ip2', 'ip3', 'ip4']] = dfZZ[['ip1', 'ip2', 'ip3', 'ip4']].astype(int)\n",
    "\n",
    "print(dfZZ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
